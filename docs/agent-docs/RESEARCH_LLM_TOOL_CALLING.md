---
title: "LLM Tool Calling Capabilities Research"
type: RESEARCH
id: "research-llm-tool-calling-2025"
created: 2025-12-02
updated: 2025-12-02
agent: copilot
model: claude-opus-4.5
requested_by: user
summary: |
  Comprehensive research on tool calling capabilities, best practices,
  token limits, and context windows for major LLM providers including
  Anthropic Claude, Google Gemini, OpenAI GPT/Codex, and xAI Grok.
keywords: [tool-calling, function-calling, llm, claude, gemini, gpt, grok, codex, mcp]
tags: ["#ai-agents", "#tool-calling", "#research", "#git-core-protocol"]
topics: [ai-agents, tool-calling, llm-capabilities]
related_issues: []
project: Git-Core-Protocol
protocol_version: 1.5.0
module: agent-docs
language: markdown
priority: high
status: complete
confidence: 0.95
token_estimate: 4500
complexity: high
---

# üîß LLM Tool Calling Capabilities Research

> **Research Date:** December 2, 2025
> **Purpose:** Compare tool/function calling capabilities across major LLM providers for Git-Core Protocol agent integration

---

## üìä Quick Comparison Table

| Feature | Claude 4.5 (Anthropic) | Gemini 3 Pro (Google) | GPT-5.1 Codex (OpenAI) | Grok 4.1 Fast (xAI) |
|---------|------------------------|----------------------|------------------------|---------------------|
| **Context Window** | 200K (1M beta) | 1M+ tokens | Not disclosed | 2M tokens |
| **Max Output** | 64K tokens | Varies | Not disclosed | Not disclosed |
| **Tool Calling** | ‚úÖ Native | ‚úÖ Native | ‚úÖ Native | ‚úÖ Native |
| **Parallel Tools** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |
| **MCP Support** | ‚úÖ Native | ‚úÖ Native | ‚úÖ Native | ‚úÖ Remote MCP |
| **Structured Output** | ‚úÖ Strict mode | ‚úÖ Yes | ‚úÖ JSON mode | ‚úÖ Yes |
| **Reasoning** | ‚úÖ Extended thinking | ‚úÖ Thinking signatures | ‚úÖ Reasoning models | ‚úÖ Native |
| **Best For** | Complex agents, coding | Multi-modal, long context | Agentic coding | Fast tool calling |

---

## 1. üü£ Claude 4.5 Models (Anthropic)

### Current Models (as of Dec 2025)

| Model | API ID | Pricing (MTok) | Context | Max Output | Best For |
|-------|--------|----------------|---------|------------|----------|
| **Sonnet 4.5** | `claude-sonnet-4-5-20250929` | $3/$15 | 200K (1M beta) | 64K | Complex agents, coding |
| **Haiku 4.5** | `claude-haiku-4-5-20251001` | $1/$5 | 200K | 64K | Fast, cost-effective |
| **Opus 4.5** | `claude-opus-4-5-20251101` | $5/$25 | 200K | 64K | Maximum intelligence |
| **Opus 4.1** | `claude-opus-4-1-20250805` | $15/$75 | 200K | 32K | Specialized reasoning |

### Tool Calling Syntax

```json
{
  "model": "claude-sonnet-4-5",
  "max_tokens": 1024,
  "tools": [
    {
      "name": "get_weather",
      "description": "Get the current weather in a given location",
      "input_schema": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA"
          }
        },
        "required": ["location"]
      }
    }
  ],
  "messages": [
    {"role": "user", "content": "What is the weather like in San Francisco?"}
  ]
}
```

### Best Practices for Claude

1. **System Prompts:**
   - Place critical instructions at the beginning
   - Use XML tags for structure: `<instructions>`, `<context>`, `<output_format>`
   - Be explicit about tool usage expectations

2. **Tool Descriptions:**
   - Write clear, detailed descriptions (the model relies heavily on these)
   - Include example values in parameter descriptions
   - Use `strict: true` for guaranteed schema conformance

3. **Context Management:**
   - Leverage 1M context beta for large codebases
   - Use prompt caching for repeated prompts (significant cost savings)
   - Extended thinking available for complex reasoning

### Strengths

- ‚úÖ Excellent at following complex instructions
- ‚úÖ Strong coding capabilities
- ‚úÖ Native MCP support
- ‚úÖ Strict tool use for schema validation
- ‚úÖ Extended thinking for multi-step reasoning

### Weaknesses

- ‚ö†Ô∏è Higher cost for Opus models
- ‚ö†Ô∏è 1M context still in beta
- ‚ö†Ô∏è May be verbose without explicit constraints

---

## 2. üîµ Gemini 3 Pro (Google)

### Current Models (as of Dec 2025)

| Model | Context Window | Features |
|-------|---------------|----------|
| **Gemini 3 Pro** | 1M+ tokens | Best multimodal, agentic, coding |
| **Gemini 2.5 Flash** | 1M tokens | Fast, price-performance |
| **Gemini 2.5 Pro** | 1M tokens | Advanced thinking |

### Tool Calling Syntax

```python
from google import genai
from google.genai import types

# Define function declaration
get_weather_function = {
    "name": "get_current_weather",
    "description": "Get the current weather in a given location",
    "parameters": {
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA"
            },
            "unit": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"],
                "description": "Temperature unit"
            }
        },
        "required": ["location"]
    }
}

# Configure tools
client = genai.Client()
tools = types.Tool(function_declarations=[get_weather_function])
config = types.GenerateContentConfig(tools=[tools])

# Send request
response = client.models.generate_content(
    model="gemini-3-pro",
    contents="What's the weather in London?",
    config=config
)
```

### Function Calling Modes

| Mode | Behavior |
|------|----------|
| `AUTO` (default) | Model decides when to use tools |
| `ANY` | Force tool use, guaranteed schema compliance |
| `NONE` | Disable function calling |
| `VALIDATED` | Force compliance, allow text or function |

### Best Practices for Gemini

1. **Thinking Signatures (Gemini 3):**
   - Gemini 3 uses thought signatures for multi-turn context
   - SDK handles this automatically
   - For manual management, preserve `thought_signature` in responses

2. **Parallel Function Calling:**

   ```python
   # Multiple functions called in one response
   config = types.GenerateContentConfig(
       tools=house_tools,
       tool_config=types.ToolConfig(
           function_calling_config=types.FunctionCallingConfig(mode='ANY')
       )
   )
   ```

3. **Automatic Function Calling (Python SDK):**
   - Pass Python functions directly as tools
   - SDK handles execution loop automatically

### Strengths

- ‚úÖ Massive 1M+ token context window
- ‚úÖ Native MCP support
- ‚úÖ Automatic function calling in Python SDK
- ‚úÖ Parallel and compositional function calling
- ‚úÖ Strong multimodal capabilities

### Weaknesses

- ‚ö†Ô∏è Thought signatures add complexity for manual implementations
- ‚ö†Ô∏è Auto function calling only in Python SDK
- ‚ö†Ô∏è Limited OpenAPI schema subset support

---

## 3. üü¢ GPT-5.1 Codex (OpenAI)

### Current Models (as of Dec 2025)

| Model | Best For | Pricing (MTok) |
|-------|----------|----------------|
| **gpt-5.1-codex-max** | Long-horizon agentic coding | Via ChatGPT credits |
| **gpt-5.1-codex-mini** | Cost-effective coding | $0.25/$2.00 |
| **gpt-5.1** | General coding/agentic | $1.25/$10.00 |
| **gpt-5** | General reasoning | $1.25/$10.00 |

### Tool Calling via Responses API

```python
from openai import OpenAI

client = OpenAI()

tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "City and state, e.g. San Francisco, CA"
                    }
                },
                "required": ["location"]
            }
        }
    }
]

response = client.chat.completions.create(
    model="gpt-5.1",
    messages=[{"role": "user", "content": "What's the weather in NYC?"}],
    tools=tools,
    tool_choice="auto"
)
```

### Codex CLI Best Practices

1. **Use AGENTS.md Files:**

   ```bash
   codex /init  # Create AGENTS.md with project context
   ```

2. **Provide Clear Code Pointers:**
   - Use greppable identifiers
   - Include full stack traces
   - Reference specific files/packages

3. **Include Verification Steps:**
   - Provide steps to reproduce issues
   - Include test commands
   - Specify linter/pre-commit checks

4. **Split Large Tasks:**
   - Break complex work into smaller focused steps
   - Easier for Codex to test and verify

### Built-in Tools

| Tool | Pricing | Description |
|------|---------|-------------|
| Code Interpreter | $0.03/session | Python execution environment |
| File Search | $0.10/GB/day | Vector storage search |
| Web Search | $10/1000 calls | Internet search |
| Apply Patch | Built-in | Code patching |
| Shell | Built-in | Command execution |

### Strengths

- ‚úÖ Purpose-built for coding tasks
- ‚úÖ CLI and IDE integration
- ‚úÖ Cloud tasks for background work
- ‚úÖ AGENTS.md native support
- ‚úÖ Strong agentic capabilities

### Weaknesses

- ‚ö†Ô∏è Context window not publicly disclosed
- ‚ö†Ô∏è Premium features require ChatGPT subscription
- ‚ö†Ô∏è Windows support experimental (WSL recommended)

---

## 4. ‚ö´ Grok 4.1 Fast (xAI)

### Current Models (as of Dec 2025)

| Model | Context Window | Features |
|-------|---------------|----------|
| **Grok 4.1 Fast** | 2,000,000 tokens | Optimized for agentic tool calling |
| **Grok 4** | Large | Reasoning model (no non-reasoning mode) |

### Tool Calling Syntax

```python
from xai_sdk import Client
from xai_sdk.chat import tool, tool_result, user

client = Client(api_key=os.getenv('XAI_API_KEY'))

# Define tools with Pydantic
from pydantic import BaseModel, Field

class TemperatureRequest(BaseModel):
    location: str = Field(description="The city and state, e.g. San Francisco, CA")
    unit: Literal["celsius", "fahrenheit"] = Field("fahrenheit")

def get_current_temperature(request: TemperatureRequest):
    return {"location": request.location, "temperature": 59, "unit": request.unit}

# Create tool definitions
tool_definitions = [
    tool(
        name="get_current_temperature",
        description="Get the current temperature in a given location",
        parameters=TemperatureRequest.model_json_schema()
    )
]

# Create chat
chat = client.chat.create(model="grok-4", tools=tool_definitions)
chat.append(user("What's the temperature in San Francisco?"))
response = chat.sample()
```

### Function Calling Modes

| Mode | Behavior |
|------|----------|
| `auto` (default) | Model decides when to call functions |
| `required` | Force function call (may hallucinate parameters) |
| `none` | Disable function calling |
| `{"type": "function", "function": {"name": "..."}}` | Force specific function |

### Server-Side Tools (Free until Dec 3, 2025)

| Tool | Price | Description |
|------|-------|-------------|
| Web Search | $5/1k | Internet search |
| X Search | $5/1k | X posts, users, threads |
| Code Execution | $5/1k | Python environment |
| Document Search | $5/1k | Uploaded files search |
| Collections Search | $2.50/1k | Knowledge base search |
| Remote MCP Tools | Token-based | Custom MCP tools |

### Best Practices for Grok

1. **Pydantic for Tool Definitions:**
   - Use Pydantic models for type safety
   - Auto-generate JSON schema from models

2. **Parallel Function Calling:**
   - Enabled by default
   - Disable with `parallel_function_calling: "false"`

3. **No Role Order Limitation:**
   - Mix `system`, `user`, `assistant` roles freely

### Strengths

- ‚úÖ Massive 2M token context window
- ‚úÖ Lightning fast for tool calling
- ‚úÖ Native X/Twitter integration
- ‚úÖ Flexible message role ordering
- ‚úÖ Remote MCP support

### Weaknesses

- ‚ö†Ô∏è Grok 4 is reasoning-only (no non-reasoning mode)
- ‚ö†Ô∏è Some parameters not supported (`presencePenalty`, `frequencyPenalty`, `stop`)
- ‚ö†Ô∏è Knowledge cutoff November 2024

---

## üéØ Recommendations for Git-Core Protocol

### Agent Selection by Task

| Task Type | Primary Agent | Secondary |
|-----------|---------------|-----------|
| **Complex Reasoning** | Claude Opus 4.5 | Gemini 3 Pro |
| **Fast Coding Tasks** | GPT-5.1 Codex | Grok 4.1 Fast |
| **Long Context Analysis** | Gemini 3 Pro | Grok 4.1 Fast |
| **Cost-Effective** | Claude Haiku 4.5 | GPT-5.1-codex-mini |
| **Tool-Heavy Workflows** | Grok 4.1 Fast | Claude Sonnet 4.5 |
| **Multi-Modal** | Gemini 3 Pro | Claude Sonnet 4.5 |

### Tool Definition Template (Cross-Platform)

```json
{
  "name": "tool_name",
  "description": "Clear, detailed description with examples",
  "parameters": {
    "type": "object",
    "properties": {
      "param1": {
        "type": "string",
        "description": "Description with format examples"
      },
      "param2": {
        "type": "string",
        "enum": ["option1", "option2"],
        "description": "Constrained values"
      }
    },
    "required": ["param1"]
  }
}
```

### Best Practices Summary

| Practice | All Models |
|----------|-----------|
| **Descriptive Names** | Use snake_case, no spaces/special chars |
| **Rich Descriptions** | Include examples, constraints, formats |
| **Type Safety** | Use enums for constrained values |
| **Error Handling** | Return informative error messages |
| **Limit Tools** | 10-20 active tools max for best results |
| **Low Temperature** | Use 0-0.3 for deterministic tool calls |
| **Validation** | Validate user confirmation for critical actions |

### MCP Integration

All major models now support MCP (Model Context Protocol):

```python
# Generic MCP pattern
from mcp import ClientSession, StdioServerParameters

server_params = StdioServerParameters(
    command="npx",
    args=["-y", "@your/mcp-server"],
)

async with stdio_client(server_params) as (read, write):
    async with ClientSession(read, write) as session:
        await session.initialize()
        # Use session as tool source
```

---

## üìö References

- [Anthropic Claude Tool Use](https://platform.claude.com/docs/en/build-with-claude/tool-use)
- [Google Gemini Function Calling](https://ai.google.dev/gemini-api/docs/function-calling)
- [OpenAI Codex Documentation](https://developers.openai.com/codex)
- [xAI Grok Function Calling](https://docs.x.ai/docs/guides/function-calling)
- [Model Context Protocol](https://modelcontextprotocol.io/)

---

*Last Updated: December 2, 2025*
